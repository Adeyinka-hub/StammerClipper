{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, options,inp_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Reading parameters\n",
    "        self.input_dim=inp_dim\n",
    "        self.lstm_lay=list(map(int, options['lstm_lay'].split(',')))\n",
    "        self.lstm_drop=list(map(float, options['lstm_drop'].split(','))) \n",
    "        self.lstm_use_batchnorm=list(map(strtobool, options['lstm_use_batchnorm'].split(',')))\n",
    "        self.lstm_use_laynorm=list(map(strtobool, options['lstm_use_laynorm'].split(','))) \n",
    "        self.lstm_use_laynorm_inp=strtobool(options['lstm_use_laynorm_inp'])\n",
    "        self.lstm_use_batchnorm_inp=strtobool(options['lstm_use_batchnorm_inp'])\n",
    "        self.lstm_act=options['lstm_act'].split(',')\n",
    "        self.lstm_orthinit=strtobool(options['lstm_orthinit'])\n",
    "\n",
    "        self.bidir=strtobool(options['lstm_bidir'])\n",
    "        self.use_cuda=strtobool(options['use_cuda'])\n",
    "        self.to_do=options['to_do']\n",
    "        \n",
    "        if self.to_do=='train':\n",
    "            self.test_flag=False\n",
    "        else:\n",
    "            self.test_flag=True\n",
    "        \n",
    "        \n",
    "        # List initialization\n",
    "        self.wfx  = nn.ModuleList([]) # Forget\n",
    "        self.ufh  = nn.ModuleList([]) # Forget\n",
    "        \n",
    "        self.wix  = nn.ModuleList([]) # Input\n",
    "        self.uih  = nn.ModuleList([]) # Input  \n",
    "        \n",
    "        self.wox  = nn.ModuleList([]) # Output\n",
    "        self.uoh  = nn.ModuleList([]) # Output  \n",
    "        \n",
    "        self.wcx  = nn.ModuleList([]) # Cell state\n",
    "        self.uch = nn.ModuleList([])  # Cell state\n",
    "        \n",
    "        self.ln  = nn.ModuleList([]) # Layer Norm\n",
    "        self.bn_wfx  = nn.ModuleList([]) # Batch Norm\n",
    "        self.bn_wix  = nn.ModuleList([]) # Batch Norm\n",
    "        self.bn_wox  = nn.ModuleList([]) # Batch Norm\n",
    "        self.bn_wcx = nn.ModuleList([]) # Batch Norm\n",
    "        \n",
    "        self.act  = nn.ModuleList([]) # Activations\n",
    "       \n",
    "  \n",
    "        # Input layer normalization\n",
    "        if self.lstm_use_laynorm_inp:\n",
    "            self.ln0=LayerNorm(self.input_dim)\n",
    "          \n",
    "        # Input batch normalization    \n",
    "        if self.lstm_use_batchnorm_inp:\n",
    "            self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n",
    "           \n",
    "        self.N_lstm_lay=len(self.lstm_lay)\n",
    "             \n",
    "        current_input=self.input_dim\n",
    "        \n",
    "        # Initialization of hidden layers\n",
    "        \n",
    "        for i in range(self.N_lstm_lay):\n",
    "             \n",
    "             # Activations\n",
    "             self.act.append(act_fun(self.lstm_act[i]))\n",
    "            \n",
    "             add_bias=True\n",
    "             \n",
    "             if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n",
    "                add_bias=False\n",
    "             \n",
    "                  \n",
    "             # Feed-forward connections\n",
    "            self.wfx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n",
    "            self.wix.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n",
    "            self.wox.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n",
    "            self.wcx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n",
    "            \n",
    "             # Recurrent connections\n",
    "            self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n",
    "            self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n",
    "            self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n",
    "            self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n",
    "             \n",
    "            if self.lstm_orthinit:\n",
    "                nn.init.orthogonal_(self.ufh[i].weight)\n",
    "                nn.init.orthogonal_(self.uih[i].weight)\n",
    "                nn.init.orthogonal_(self.uoh[i].weight)\n",
    "                nn.init.orthogonal_(self.uch[i].weight)\n",
    "            \n",
    "             \n",
    "             # batch norm initialization\n",
    "            self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n",
    "            self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n",
    "            self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n",
    "            self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n",
    "                \n",
    "            self.ln.append(LayerNorm(self.lstm_lay[i]))\n",
    "                \n",
    "            if self.bidir:\n",
    "                current_input=2*self.lstm_lay[i]\n",
    "            else:\n",
    "                current_input=self.lstm_lay[i]\n",
    "                 \n",
    "        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n",
    "            \n",
    "             \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applying Layer/Batch Norm\n",
    "        if bool(self.lstm_use_laynorm_inp):\n",
    "            x=self.ln0((x))\n",
    "        \n",
    "        if bool(self.lstm_use_batchnorm_inp):\n",
    "            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n",
    "            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n",
    "\n",
    "          \n",
    "        for i in range(self.N_lstm_lay):\n",
    "            \n",
    "            # Initial state and concatenation\n",
    "            if self.bidir:\n",
    "                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n",
    "                x=torch.cat([x,flip(x,0)],1)\n",
    "            else:\n",
    "                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n",
    "        \n",
    "               \n",
    "            # Drop mask initilization (same mask for all time steps)            \n",
    "            if self.test_flag==False:\n",
    "                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n",
    "            else:\n",
    "                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                h_init=h_init.cuda()\n",
    "                drop_mask=drop_mask.cuda()\n",
    "               \n",
    "                 \n",
    "            # Feed-forward affine transformations (all steps in parallel)\n",
    "            wfx_out=self.wfx[i](x)\n",
    "            wix_out=self.wix[i](x)\n",
    "            wox_out=self.wox[i](x)\n",
    "            wcx_out=self.wcx[i](x)\n",
    "            \n",
    "            # Apply batch norm if needed (all steos in parallel)\n",
    "            if self.lstm_use_batchnorm[i]:\n",
    "\n",
    "                wfx_out_bn=self.bn_wfx[i](wfx_out.view(wfx_out.shape[0]*wfx_out.shape[1],wfx_out.shape[2]))\n",
    "                wfx_out=wfx_out_bn.view(wfx_out.shape[0],wfx_out.shape[1],wfx_out.shape[2])\n",
    "         \n",
    "                wix_out_bn=self.bn_wix[i](wix_out.view(wix_out.shape[0]*wix_out.shape[1],wix_out.shape[2]))\n",
    "                wix_out=wix_out_bn.view(wix_out.shape[0],wix_out.shape[1],wix_out.shape[2])\n",
    "   \n",
    "                wox_out_bn=self.bn_wox[i](wox_out.view(wox_out.shape[0]*wox_out.shape[1],wox_out.shape[2]))\n",
    "                wox_out=wox_out_bn.view(wox_out.shape[0],wox_out.shape[1],wox_out.shape[2])\n",
    "\n",
    "                wcx_out_bn=self.bn_wcx[i](wcx_out.view(wcx_out.shape[0]*wcx_out.shape[1],wcx_out.shape[2]))\n",
    "                wcx_out=wcx_out_bn.view(wcx_out.shape[0],wcx_out.shape[1],wcx_out.shape[2]) \n",
    "            \n",
    "            \n",
    "            # Processing time steps\n",
    "            hiddens = []\n",
    "            ct=h_init\n",
    "            ht=h_init\n",
    "            \n",
    "            for k in range(x.shape[0]):\n",
    "                \n",
    "                # LSTM equations\n",
    "                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n",
    "                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n",
    "                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n",
    "                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n",
    "                ht=ot*self.act[i](ct)\n",
    "                \n",
    "                if self.lstm_use_laynorm[i]:\n",
    "                    ht=self.ln[i](ht)\n",
    "                    \n",
    "                hiddens.append(ht)\n",
    "                \n",
    "            # Stacking hidden states\n",
    "            h=torch.stack(hiddens)\n",
    "            \n",
    "            # Bidirectional concatenations\n",
    "            if self.bidir:\n",
    "                h_f=h[:,0:int(x.shape[1]/2)]\n",
    "                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n",
    "                h=torch.cat([h_f,h_b],2)\n",
    "                \n",
    "            # Setup x for the next hidden layer\n",
    "            x=h\n",
    "\n",
    "              \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
