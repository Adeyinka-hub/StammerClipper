{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The config file -f does not exist!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################################\n",
    "# pytorch-kaldi v.0.1                                      \n",
    "# Mirco Ravanelli, Titouan Parcollet\n",
    "# Mila, University of Montreal\n",
    "# October 2018\n",
    "##########################################################\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import configparser\n",
    "import numpy as np\n",
    "from utils import check_cfg,create_chunks,write_cfg_chunk,compute_avg_performance, \\\n",
    "                  read_args_command_line, run_shell,compute_n_chunks, get_all_archs,cfg_item2sec, \\\n",
    "                  dump_epoch_results, run_shell_display, create_curves\n",
    "from shutil import copyfile\n",
    "import re\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# Reading global cfg file (first argument-mandatory file) \n",
    "cfg_file=sys.argv[1]\n",
    "if not(os.path.exists(cfg_file)):\n",
    "    \n",
    "    sys.stderr.write('ERROR: The config file %s does not exist!\\n'%(cfg_file))\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(cfg_file)\n",
    "\n",
    "\n",
    "# Output folder creation\n",
    "out_folder=config['exp']['out_folder']\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder+'/exp_files')\n",
    "\n",
    "# Log file path    \n",
    "log_file=config['exp']['out_folder']+'/log.log'\n",
    "\n",
    "\n",
    "# Reading and parsing optional arguments from command line (e.g.,--optimization,lr=0.002)\n",
    "[section_args,field_args,value_args]=read_args_command_line(sys.argv,config)\n",
    "    \n",
    "# Read, parse, and check the config file     \n",
    "cfg_file_proto=config['cfg_proto']['cfg_proto']\n",
    "[config,name_data,name_arch]=check_cfg(cfg_file,config,cfg_file_proto)\n",
    "print(\"- Reading config file......OK!\")\n",
    "\n",
    "     \n",
    "# Copy the global cfg file into the output folder\n",
    "cfg_file=out_folder+'/conf.cfg'\n",
    "with open(cfg_file, 'w') as configfile:   \n",
    "    config.write(configfile) \n",
    "    \n",
    "\n",
    "# Splitting data into chunks (see out_folder/additional_files)\n",
    "create_chunks(config)\n",
    "\n",
    "print(\"- Chunk creation......OK!\\n\")\n",
    "\n",
    "# create res_file\n",
    "res_file_path=out_folder+'/res.res'\n",
    "res_file = open(res_file_path, \"w\")\n",
    "res_file.close()\n",
    "\n",
    "\n",
    "# Read cfg file options\n",
    "is_production=strtobool(config['exp']['production'])\n",
    "cfg_file_proto_chunk=config['cfg_proto']['cfg_proto_chunk']\n",
    "run_nn_script=config['exp']['run_nn_script']\n",
    "cmd=config['exp']['cmd']\n",
    "N_ep=int(config['exp']['N_epochs_tr'])\n",
    "tr_data_lst=config['data_use']['train_with'].split(',')\n",
    "valid_data_lst=config['data_use']['valid_with'].split(',')\n",
    "forward_data_lst=config['data_use']['forward_with'].split(',')\n",
    "max_seq_length_train=int(config['batches']['max_seq_length_train'])\n",
    "forward_save_files=list(map(strtobool,config['forward']['save_out_file'].split(',')))\n",
    "\n",
    "\n",
    "# Learning rates and architecture-specific optimization parameters\n",
    "arch_lst=get_all_archs(config)\n",
    "lr={}\n",
    "improvement_threshold={}\n",
    "halving_factor={}\n",
    "pt_files={}\n",
    "\n",
    "for arch in arch_lst:\n",
    "    lr[arch]=float(config[arch]['arch_lr'])\n",
    "    improvement_threshold[arch]=float(config[arch]['arch_improvement_threshold'])\n",
    "    halving_factor[arch]=float(config[arch]['arch_halving_factor'])\n",
    "    pt_files[arch]=config[arch]['arch_pretrain_file']\n",
    "\n",
    "if strtobool(config['batches']['increase_seq_length_train']):\n",
    "    max_seq_length_train_curr=int(config['batches']['start_seq_len_train'])\n",
    "else:\n",
    "    max_seq_length_train_curr=max_seq_length_train\n",
    "    \n",
    "# If production, skip training and forward directly from last saved models\n",
    "if is_production:\n",
    "    ep           = N_ep-1\n",
    "    N_ep         = 0\n",
    "    model_files  = {}\n",
    "\n",
    "    for arch in pt_files.keys():\n",
    "        model_files[arch] = out_folder+'/exp_files/final_'+arch+'.pkl'\n",
    "\n",
    "# --------TRAINING LOOP--------#\n",
    "for ep in range(N_ep):\n",
    "    \n",
    "    tr_loss_tot=0\n",
    "    tr_error_tot=0\n",
    "    tr_time_tot=0\n",
    "    \n",
    "    print('------------------------------ Epoch %s / %s ------------------------------'%(format(ep, \"03d\"),format(N_ep-1, \"03d\")))\n",
    "\n",
    "    for tr_data in tr_data_lst:\n",
    "        \n",
    "        # Compute the total number of chunks for each training epoch\n",
    "        N_ck_tr=compute_n_chunks(out_folder,tr_data,ep,'train')\n",
    "    \n",
    "     \n",
    "        # ***Epoch training***\n",
    "        for ck in range(N_ck_tr):\n",
    "            \n",
    "            # path of the list of features for this chunk\n",
    "            lst_file=out_folder+'/exp_files/train_'+tr_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'_*.lst'\n",
    "            \n",
    "            # paths of the output files (info,model,chunk_specific cfg file)\n",
    "            info_file=out_folder+'/exp_files/train_'+tr_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.info'\n",
    "            \n",
    "            if ep+ck==0:\n",
    "                model_files_past={}\n",
    "            else:\n",
    "                model_files_past=model_files\n",
    "                \n",
    "            model_files={}\n",
    "            for arch in pt_files.keys():\n",
    "                model_files[arch]=info_file.replace('.info','_'+arch+'.pkl')\n",
    "            \n",
    "            config_chunk_file=out_folder+'/exp_files/train_'+tr_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.cfg'\n",
    "                        \n",
    "            # Write chunk-specific cfg file\n",
    "            write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,pt_files,lst_file,info_file,'train',tr_data,lr,max_seq_length_train_curr,name_data,ep,ck)\n",
    "\n",
    "            \n",
    "            # if this chunk has not already been processed, do training...\n",
    "            if not(os.path.exists(info_file)):\n",
    "                \n",
    "                    print('Training %s chunk = %i / %i' %(tr_data,ck+1, N_ck_tr))\n",
    "    \n",
    "                    # Doing training\n",
    "                    cmd_chunk=cmd+'python ' + run_nn_script + ' ' + config_chunk_file + ' 2> ' + log_file\n",
    "                    run_shell_display(cmd_chunk)\n",
    "                    \n",
    "                    if not(os.path.exists(info_file)):\n",
    "                        sys.stderr.write(\"ERROR: training epoch %i, chunk %i not done! File %s does not exist.\\nSee %s \\n\" % (ep,ck,info_file,log_file))\n",
    "                        sys.exit(0)\n",
    "                                  \n",
    "                      \n",
    "            # update pt_file (used to initialized the DNN for the next chunk)  \n",
    "            for pt_arch in pt_files.keys():\n",
    "                pt_files[pt_arch]=out_folder+'/exp_files/train_'+tr_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'_'+pt_arch+'.pkl'\n",
    "                \n",
    "            # remove previous pkl files\n",
    "            if len(model_files_past.keys())>0:\n",
    "                for pt_arch in pt_files.keys():\n",
    "                    if os.path.exists(model_files_past[pt_arch]):\n",
    "                        os.remove(model_files_past[pt_arch]) \n",
    "    \n",
    "    \n",
    "        # Training Loss and Error    \n",
    "        tr_info_lst=sorted(glob.glob(out_folder+'/exp_files/train_'+tr_data+'_ep'+format(ep, \"03d\")+'*.info'))\n",
    "        [tr_loss,tr_error,tr_time]=compute_avg_performance(tr_info_lst)\n",
    "        \n",
    "        tr_loss_tot=tr_loss_tot+tr_loss\n",
    "        tr_error_tot=tr_error_tot+tr_error\n",
    "        tr_time_tot=tr_time_tot+tr_time\n",
    "        \n",
    "        \n",
    "        # ***Epoch validation***\n",
    "        if ep>0:\n",
    "            # store previous-epoch results (useful for learnig rate anealling)\n",
    "            valid_peformance_dict_prev=valid_peformance_dict\n",
    "        \n",
    "        valid_peformance_dict={}  \n",
    "        tot_time=tr_time  \n",
    "    \n",
    "    \n",
    "    for valid_data in valid_data_lst:\n",
    "        \n",
    "        # Compute the number of chunks for each validation dataset\n",
    "        N_ck_valid=compute_n_chunks(out_folder,valid_data,ep,'valid')\n",
    "\n",
    "    \n",
    "        for ck in range(N_ck_valid):\n",
    "            \n",
    "            # path of the list of features for this chunk\n",
    "            lst_file=out_folder+'/exp_files/valid_'+valid_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'_*.lst'\n",
    "            \n",
    "            # paths of the output files\n",
    "            info_file=out_folder+'/exp_files/valid_'+valid_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.info'            \n",
    "            config_chunk_file=out_folder+'/exp_files/valid_'+valid_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.cfg'\n",
    "            \n",
    "            # Write chunk-specific cfg file\n",
    "            write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,model_files,lst_file,info_file,'valid',valid_data,lr,max_seq_length_train_curr,name_data,ep,ck)\n",
    "\n",
    "            # Do validation if the chunk was not already processed\n",
    "            if not(os.path.exists(info_file)):\n",
    "                print('Validating %s chunk = %i / %i' %(valid_data,ck+1,N_ck_valid))\n",
    "                    \n",
    "                # Doing eval\n",
    "                cmd_chunk=cmd+'python ' + run_nn_script + ' ' + config_chunk_file + ' 2> ' + log_file\n",
    "                run_shell_display(cmd_chunk)\n",
    "                \n",
    "                if not(os.path.exists(info_file)):\n",
    "                    sys.stderr.write(\"ERROR: validation on epoch %i, chunk %i of dataset %s not done! File %s does not exist.\\nSee %s \\n\" % (ep,ck,valid_data,info_file,log_file))\n",
    "                    sys.exit(0)\n",
    "        \n",
    "        # Compute validation performance  \n",
    "        valid_info_lst=sorted(glob.glob(out_folder+'/exp_files/valid_'+valid_data+'_ep'+format(ep, \"03d\")+'*.info'))\n",
    "        [valid_loss,valid_error,valid_time]=compute_avg_performance(valid_info_lst)\n",
    "        valid_peformance_dict[valid_data]=[valid_loss,valid_error,valid_time]\n",
    "        tot_time=tot_time+valid_time\n",
    "       \n",
    "        \n",
    "    # Print results in both res_file and stdout\n",
    "    dump_epoch_results(res_file_path, ep, tr_data_lst, tr_loss_tot, tr_error_tot, tot_time, valid_data_lst, valid_peformance_dict, lr, N_ep)\n",
    "\n",
    "    #  if needed, update sentence_length\n",
    "    if strtobool(config['batches']['increase_seq_length_train']):\n",
    "        max_seq_length_train_curr=max_seq_length_train_curr*int(config['batches']['multply_factor_seq_len_train'])\n",
    "        if max_seq_length_train_curr>max_seq_length_train:\n",
    "            max_seq_length_train_curr=max_seq_length_train\n",
    "            \n",
    "               \n",
    "        \n",
    "    # Check for learning rate annealing\n",
    "    if ep>0:\n",
    "        # computing average validation error (on all the dataset specified)\n",
    "        err_valid_mean=np.mean(np.asarray(list(valid_peformance_dict.values()))[:,1])\n",
    "        err_valid_mean_prev=np.mean(np.asarray(list(valid_peformance_dict_prev.values()))[:,1])\n",
    "        \n",
    "        for lr_arch in lr.keys():\n",
    "            if ((err_valid_mean_prev-err_valid_mean)/err_valid_mean)<improvement_threshold[lr_arch]:\n",
    "                lr[lr_arch]=lr[lr_arch]*halving_factor[lr_arch]\n",
    "\n",
    "# Training has ended, copy the last .pkl to final_arch.pkl for production\n",
    "for pt_arch in pt_files.keys():\n",
    "    if os.path.exists(model_files[pt_arch]) and not os.path.exists(out_folder+'/exp_files/final_'+pt_arch+'.pkl'):\n",
    "        copyfile(model_files[pt_arch], out_folder+'/exp_files/final_'+pt_arch+'.pkl')\n",
    "  \n",
    "                \n",
    "# --------FORWARD--------#\n",
    "for forward_data in forward_data_lst:\n",
    "    \n",
    "           \n",
    "         # Compute the number of chunks\n",
    "    N_ck_forward=compute_n_chunks(out_folder,forward_data,ep,'forward')\n",
    "        \n",
    "    for ck in range(N_ck_forward):\n",
    "            \n",
    "        if not is_production:\n",
    "            print('Testing %s chunk = %i / %i' %(forward_data,ck+1, N_ck_forward))\n",
    "        else: \n",
    "            print('Forwarding %s chunk = %i / %i' %(forward_data,ck+1, N_ck_forward))\n",
    "\n",
    "            # path of the list of features for this chunk\n",
    "            lst_file=out_folder+'/exp_files/forward_'+forward_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'_*.lst'\n",
    "            \n",
    "            # output file\n",
    "            info_file=out_folder+'/exp_files/forward_'+forward_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.info'\n",
    "            config_chunk_file=out_folder+'/exp_files/forward_'+forward_data+'_ep'+format(ep, \"03d\")+'_ck'+format(ck, \"02d\")+'.cfg'\n",
    "       \n",
    "            # Write chunk-specific cfg file\n",
    "            write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,model_files,lst_file,info_file,'forward',forward_data,lr,max_seq_length_train_curr,name_data,ep,ck)\n",
    "            \n",
    "            # Do forward if the chunk was not already processed\n",
    "            if not(os.path.exists(info_file)):\n",
    "                                \n",
    "                # Doing forward\n",
    "                cmd_chunk=cmd+'python ' + run_nn_script + ' ' + config_chunk_file + ' 2> ' + log_file\n",
    "                run_shell_display(cmd_chunk)\n",
    "            \n",
    "                if not(os.path.exists(info_file)):\n",
    "                    sys.stderr.write(\"ERROR: forward chunk %i of dataset %s not done! File %s does not exist.\\nSee %s \\n\" % (ck,forward_data,info_file,log_file))\n",
    "                    sys.exit(0)\n",
    "                    \n",
    "               \n",
    "            \n",
    "# --------DECODING--------#\n",
    "dec_lst=glob.glob( out_folder+'/exp_files/*_to_decode.ark')\n",
    "\n",
    "forward_data_lst=config['data_use']['forward_with'].split(',')\n",
    "forward_outs=config['forward']['forward_out'].split(',')\n",
    "forward_dec_outs=list(map(strtobool,config['forward']['require_decoding'].split(',')))\n",
    "\n",
    "\n",
    "for data in forward_data_lst:\n",
    "    for k in range(len(forward_outs)):\n",
    "        if forward_dec_outs[k]:\n",
    "            \n",
    "            print('Decoding %s output %s' %(data,forward_outs[k]))\n",
    "            \n",
    "            info_file=out_folder+'/exp_files/decoding_'+data+'_'+forward_outs[k]+'.info'\n",
    "            \n",
    "            \n",
    "            # create decode config file\n",
    "            config_dec_file=out_folder+'/decoding_'+data+'_'+forward_outs[k]+'.conf'\n",
    "            config_dec = configparser.ConfigParser()\n",
    "            config_dec.add_section('decoding')\n",
    "            \n",
    "            for dec_key in config['decoding'].keys():\n",
    "                config_dec.set('decoding',dec_key,config['decoding'][dec_key])\n",
    " \n",
    "            # add graph_dir, datadir, alidir\n",
    "            lab_field=config[cfg_item2sec(config,'data_name',data)]['lab']\n",
    "            \n",
    "            \n",
    "            # Production case, we don't have labels\n",
    "            if not is_production:\n",
    "                pattern='lab_folder=(.*)\\nlab_opts=(.*)\\nlab_count_file=(.*)\\nlab_data_folder=(.*)\\nlab_graph=(.*)'\n",
    "                alidir=re.findall(pattern,lab_field)[0][0]\n",
    "                config_dec.set('decoding','alidir',os.path.abspath(alidir))\n",
    "\n",
    "                datadir=re.findall(pattern,lab_field)[0][3]\n",
    "                config_dec.set('decoding','data',os.path.abspath(datadir))\n",
    "                \n",
    "                graphdir=re.findall(pattern,lab_field)[0][4]\n",
    "                config_dec.set('decoding','graphdir',os.path.abspath(graphdir))\n",
    "            else:\n",
    "                pattern='lab_data_folder=(.*)\\nlab_graph=(.*)'\n",
    "                datadir=re.findall(pattern,lab_field)[0][0]\n",
    "                config_dec.set('decoding','data',os.path.abspath(datadir))\n",
    "                \n",
    "                graphdir=re.findall(pattern,lab_field)[0][1]\n",
    "                config_dec.set('decoding','graphdir',os.path.abspath(graphdir))\n",
    "\n",
    "                # The ali dir is supposed to be in exp/model/ which is one level ahead of graphdir\n",
    "                alidir = graphdir.split('/')[0:len(graphdir.split('/'))-1]\n",
    "                alidir = \"/\".join(alidir)\n",
    "                config_dec.set('decoding','alidir',os.path.abspath(alidir))\n",
    "\n",
    "            \n",
    "            with open(config_dec_file, 'w') as configfile:\n",
    "                config_dec.write(configfile)\n",
    "             \n",
    "            out_folder=os.path.abspath(out_folder)\n",
    "            files_dec=out_folder+'/exp_files/forward_'+data+'_ep*_ck*_'+forward_outs[k]+'_to_decode.ark'\n",
    "            out_dec_folder=out_folder+'/decode_'+data+'_'+forward_outs[k]\n",
    "                \n",
    "            if not(os.path.exists(info_file)):\n",
    "                \n",
    "                # Run the decoder\n",
    "                cmd_decode=cmd+config['decoding']['decoding_script_folder'] +'/'+ config['decoding']['decoding_script']+ ' '+os.path.abspath(config_dec_file)+' '+ out_dec_folder + ' \\\"'+ files_dec + '\\\"' \n",
    "                run_shell(cmd_decode,log_file)\n",
    "                \n",
    "                # remove ark files if needed\n",
    "                if not forward_save_files[k]:\n",
    "                    list_rem=glob.glob(files_dec)\n",
    "                    for rem_ark in list_rem:\n",
    "                        os.remove(rem_ark)\n",
    "                    \n",
    "                    \n",
    "            # Print WER results and write info file\n",
    "            cmd_res='./check_res_dec.sh '+out_dec_folder\n",
    "            wers=run_shell(cmd_res,log_file).decode('utf-8')\n",
    "            res_file = open(res_file_path, \"a\")\n",
    "            res_file.write('%s\\n'%wers)\n",
    "            print(wers)\n",
    "\n",
    "# Saving Loss and Err as .txt and plotting curves\n",
    "if not is_production:\n",
    "    create_curves(out_folder, N_ep, valid_data_lst)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
